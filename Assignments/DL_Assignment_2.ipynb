{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1. What is linearly inseparable problem? What is the role of the hidden layer?**"
      ],
      "metadata": {
        "id": "ozbVn62qfqgv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Linearly Inseparable Problem:**\n",
        "\n",
        "* A linearly inseparable problem occurs when data points cannot be separated by a single straight line (or hyperplane in higher dimensions).\n",
        "* Examples of such problems include the XOR problem, where no single line can separate the outputs of a binary XOR function.\n",
        "**Role of the Hidden Layer:**\n",
        "\n",
        "* The hidden layer in a neural network introduces non-linearity, enabling the network to solve linearly inseparable problems.\n",
        "* Hidden layers allow the network to learn complex patterns and representations by transforming the input space into a higher-dimensional space where the data can become linearly separable.\n",
        "* By using activation functions (e.g., ReLU, sigmoid, tanh) in the hidden layers, the network can model intricate relationships between the inputs and outputs."
      ],
      "metadata": {
        "id": "gzFDEKo0fqlE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.  Explain XOR problem in case of a simple perceptron.**"
      ],
      "metadata": {
        "id": "PGBQB9QYfqpH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**XOR Problem in Simple Perceptron:**\n",
        "\n",
        "The XOR (exclusive OR) problem is a classic example of a linearly inseparable problem.\n",
        "The XOR function outputs true (1) if and only if the inputs differ (i.e., one is true and the other is false).\n",
        "**For inputs (A, B):**\n",
        "* (0, 0) -> 0\n",
        "* (0, 1) -> 1\n",
        "* (1, 0) -> 1\n",
        "* (1, 1) -> 0\n",
        "A simple perceptron cannot solve the XOR problem because it cannot find a linear boundary to separate the outputs. No single line can separate the true outputs (1) from the false outputs (0)."
      ],
      "metadata": {
        "id": "4ebUsRCqfqsy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.  Design a multi-layer perceptron to implement A XOR B.**"
      ],
      "metadata": {
        "id": "U35I4jDmfqwP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Designing a Multi-Layer Perceptron for XOR:\n",
        "\n",
        "A multi-layer perceptron (MLP) with at least one hidden layer can solve the XOR problem. Here‚Äôs a simple design:\n",
        "\n",
        "Input Layer: 2 neurons (A and B)\n",
        "Hidden Layer: 2 neurons with activation function (e.g., sigmoid, tanh, ReLU)\n",
        "Output Layer: 1 neuron with activation function (e.g., sigmoid)\n",
        "Architecture:\n",
        "\n",
        "Input Layer:\n",
        "\n",
        "A\n",
        "ùêµ\n",
        "\n",
        "Hidden Layer\n",
        "Neuron 1:\n",
        ""
      ],
      "metadata": {
        "id": "goRM3ViUfqzg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.  Explain the single-layer feed forward architecture of ANN.**"
      ],
      "metadata": {
        "id": "Odqj2q9HgYKZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Single-Layer Feed Forward Architecture:\n",
        "\n",
        "The single-layer feed forward neural network, also known as a perceptron, consists of an input layer and an output layer with no hidden layers.\n",
        "Components:\n",
        "Input Layer: Takes the input features.\n",
        "Output Layer: Produces the final output.\n",
        "Operation:\n",
        "Each input neuron is connected to each output neuron through weighted connections.\n",
        "The input signals are multiplied by the corresponding weights, summed up, and passed through an activation function to produce the output.\n",
        "Limitations:\n",
        "It can only solve linearly separable problems.\n",
        "Unable to model complex, non-linear relationships due to the absence of hidden layers."
      ],
      "metadata": {
        "id": "BXenlsftgYQR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Explain the competitive network architecture of ANN.**"
      ],
      "metadata": {
        "id": "UQwjselDgYUe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Competitive Network Architecture:\n",
        "\n",
        "Competitive neural networks consist of neurons that compete among themselves to be activated (win).\n",
        "Components:\n",
        "Input Layer: Takes the input features.\n",
        "Competitive Layer: Contains neurons that compete to become active.\n",
        "Operation:\n",
        "Each neuron in the competitive layer computes a value based on the input signals and its weights.\n",
        "A competition mechanism (e.g., winner-takes-all) determines which neuron becomes active, typically the one with the highest activation value.\n",
        "Applications:\n",
        "Used in clustering and pattern recognition tasks, such as Self-Organizing Maps (SOM) and Learning Vector Quantization (LVQ).\n",
        "Useful in unsupervised learning scenarios where the goal is to discover patterns and structure in the data."
      ],
      "metadata": {
        "id": "kuxmiu6cgYb-"
      }
    }
  ]
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e029598-1b79-4625-8bab-fc13d5cdeb15",
   "metadata": {},
   "source": [
    "**Question 1: Explain the differences between a Perceptron, a Multi-Layer Perceptron (MLP), and a Convolutional Neural Network (CNN). Provide examples of tasks for which each type of network is best suited.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d1e533-3298-489f-b827-90ea2c9d3314",
   "metadata": {},
   "outputs": [],
   "source": [
    "Perceptron: A perceptron is a single-layer neural network used for binary classification tasks. \n",
    "            It consists of an input layer and an output node with a step activation function. \n",
    "             It’s best suited for linearly separable problems.\n",
    "\n",
    "Multi-Layer Perceptron (MLP): An MLP is a feedforward neural network with one or more hidden layers between the input and output layers.\n",
    "                      Each neuron uses a non-linear activation function, allowing the MLP to model complex, non-linear relationships. \n",
    "                      MLPs are suited for tasks like image and text classification, where the relationship between input features is non-linear.\n",
    "\n",
    "Convolutional Neural Network (CNN): A CNN is a type of neural network specifically designed for processing grid-like data such as images. \n",
    "                                 It uses convolutional layers to detect local patterns, such as edges and textures. \n",
    "                          CNNs are best suited for image recognition, object detection, and other tasks where spatial hierarchies in data are important.\n",
    "                                                                                               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5375450-2a84-4fad-9671-d7b3b21ab2d4",
   "metadata": {},
   "source": [
    "**Question 2: Describe the process of forward propagation in a neural network. How do activation functions play a role in this process? \n",
    "Provide mathematical formulations for commonly used activation functions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b5c2ee-a194-4628-b237-82658ffbf540",
   "metadata": {},
   "outputs": [],
   "source": [
    "Forward Propagation: In forward propagation, input data is passed through the layers of the neural network, \n",
    "                    where each neuron computes a weighted sum of its inputs and applies an activation function. \n",
    "                   The result is then passed to the next layer, continuing until the output layer produces the final prediction.\n",
    "\n",
    "Role of Activation Functions: Activation functions introduce non-linearity into the network, enabling it to model complex patterns. \n",
    "                            Without them, the network would behave like a linear model regardless of its depth.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d105df-d150-4bb6-b7d3-fa4954ee5f2a",
   "metadata": {},
   "source": [
    "**Question 3: What is the vanishing gradient problem, and how does it affect the training of deep neural networks? \n",
    "Discuss potential solutions to this problem.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9b83fe-e3a2-47e1-ba08-de0c803e52fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Vanishing Gradient Problem: The vanishing gradient problem occurs during backpropagation when gradients become very small, \n",
    "                           causing the weights in the earlier layers of the network to update very slowly. \n",
    "                           This slows down or even halts the learning process, particularly in deep networks with many layers.\n",
    "\n",
    "Impact: As a result, the network may fail to learn useful features from the data, especially in the initial layers.\n",
    "\n",
    "Solutions:\n",
    "Use of ReLU Activation Function: Unlike sigmoid or tanh, ReLU does not saturate for positive inputs, reducing the likelihood of vanishing gradients.\n",
    "Gradient Clipping: Limiting the size of gradients during backpropagation to prevent them from becoming too small.\n",
    "Initialization Techniques: Using methods like Xavier or He initialization to set the initial weights in a way that mitigates the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b1574f-817b-4101-9d62-60b72e7e2094",
   "metadata": {},
   "source": [
    "**Question 4: Compare and contrast Stochastic Gradient Descent (SGD), Mini-Batch Gradient Descent, \n",
    "and Batch Gradient Descent. How do these methods affect the convergence of a neural network during training?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b8e3e3-5c71-4322-9544-c7a6a72b7024",
   "metadata": {},
   "outputs": [],
   "source": [
    "Batch Gradient Descent: Computes the gradient of the cost function with respect to the entire training dataset. \n",
    "                      While accurate, it can be computationally expensive and slow for large datasets.\n",
    "\n",
    "Stochastic Gradient Descent (SGD): Updates the weights for each training example one at a time, leading to faster updates \n",
    "                                   and potential for escaping local minima. \n",
    "                            However, it introduces more noise into the gradient estimation, which can cause fluctuations in the optimization process.\n",
    "\n",
    "Mini-Batch Gradient Descent: A compromise between batch and SGD, mini-batch gradient descent updates \n",
    "                           the weights based on a small random subset (mini-batch) of the data.\n",
    "                            It balances the stability of batch gradient descent with the speed of SGD.\n",
    "\n",
    "Convergence:\n",
    "SGD can converge faster due to frequent updates but with more oscillations.\n",
    "Batch Gradient Descent is more stable but slower due to its computation on the entire dataset.\n",
    "Mini-Batch Gradient Descent generally provides a good balance, often leading to faster and smoother convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8de701f-fe70-4d6d-81ef-1564e32471e5",
   "metadata": {},
   "source": [
    "**Question 5: Discuss the concept of overfitting in the context of deep learning. What are some techniques used to prevent overfitting in deep neural networks?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc36c70-e17b-47b5-a810-2a04d4a43831",
   "metadata": {},
   "source": [
    "Overfitting: Occurs when a model learns not only the underlying patterns in the training data but \n",
    "            also the noise and outliers, leading to poor generalization on new, unseen data.\n",
    "\n",
    "Prevention Techniques:\n",
    "Regularization: Adding a penalty to the loss function for large weights (e.g., L2 regularization) to constrain the model’s complexity.\n",
    "\n",
    "Dropout: Randomly setting a fraction of the neurons to zero during training, preventing the model from becoming too reliant on any single neuron.\n",
    "\n",
    "Early Stopping: Monitoring the model's performance on a validation set during training and stopping once the performance starts to degrade, indicating overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
